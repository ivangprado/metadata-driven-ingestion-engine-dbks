{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf72490-011e-48d6-84c3-42a670558011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef15d478-74ba-44c1-b37e-f4d8eaea67ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def connect_rest_api(spark, url: str, query:str):\n",
    "\n",
    "    # Ensure query starts with '?', but only if it exists\n",
    "    full_url = url + query if query.startswith(\"?\") else url + \"?\" + query if query else url\n",
    "\n",
    "    response = requests.get(full_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    json_data = response.json()\n",
    "\n",
    "    if isinstance(json_data, dict):\n",
    "        for value in json_data.values():\n",
    "            if isinstance(value, list):\n",
    "                return spark.read.json(spark.sparkContext.parallelize(value))\n",
    "        return spark.createDataFrame([json_data])\n",
    "\n",
    "    if isinstance(json_data, list):\n",
    "        return spark.read.json(spark.sparkContext.parallelize(json_data))\n",
    "\n",
    "    raise Exception(\"Unsupported API JSON structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01ad4954-1a9e-415a-9b29-2ce885549bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def connect_jdbc(spark, connectorstring: str, query: str, driver: str = None):\n",
    "    reader = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", connectorstring) \\\n",
    "        .option(\"query\", query)\n",
    "    if driver:\n",
    "        reader = reader.option(\"driver\", driver)\n",
    "    return reader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d1b518d-d604-43a9-b515-83b3e317052b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def connect_olap(spark, sql_server_url, openquery_mdx, username, password):\n",
    "    mdx_clean = openquery_mdx.strip().replace(\"'\", \"''\")\n",
    "    openquery_sql = f\"\"\"\n",
    "    (\n",
    "        SELECT * FROM OPENQUERY(\n",
    "            SSAS,\n",
    "            '{mdx_clean}'\n",
    "        )\n",
    "    ) AS mdx_result\n",
    "    \"\"\"\n",
    "\n",
    "    connection_properties = {\n",
    "        \"user\": username,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "    }\n",
    "\n",
    "    return spark.read.jdbc(\n",
    "        url=sql_server_url,\n",
    "        table=openquery_sql,\n",
    "        properties=connection_properties\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "connectors",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
